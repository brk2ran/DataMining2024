{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# 1. Datensatz laden\n",
    "X_train = pd.read_csv('X_train.csv')\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "y_test = pd.read_csv('y_test.csv')\n",
    "\n",
    "\n",
    "# y_train und y_test in Series umwandeln\n",
    "y_train = pd.read_csv('y_train.csv').iloc[:, 0]\n",
    "y_test = pd.read_csv('y_test.csv').iloc[:, 0]\n",
    "\n",
    "# 2. Angepasste RMSPE-Funktion\n",
    "def rmspe(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    y_true_filtered = y_true[mask]\n",
    "    y_pred_filtered = y_pred[mask]\n",
    "    return np.sqrt(np.mean(((y_true_filtered - y_pred_filtered) / y_true_filtered) ** 2))\n",
    "\n",
    "# RMSPE als Scorer definieren\n",
    "rmspe_scorer = make_scorer(rmspe, greater_is_better=False)\n",
    "\n",
    "# 3. Numerische und kategorische Features definieren\n",
    "numerical_features = ['year', 'month', 'day', 'week_of_year', 'lag_1', 'lag_7']\n",
    "already_encoded_features = ['Open', 'Promo', 'promo2']\n",
    "categorical_features_to_encode = ['Store', 'DayOfWeek', 'StoreType', 'StateHoliday', 'Assortment']\n",
    "\n",
    "# 4. Preprocessor erstellen\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features + already_encoded_features),  \n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features_to_encode)  \n",
    "    ])\n",
    "\n",
    "# 5. Pipeline erstellen\n",
    "pipeline_lr = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# 6. Pipeline mit den Trainingsdaten trainieren\n",
    "pipeline_lr.fit(X_train, y_train)\n",
    "\n",
    "# 7. Cross-Validation mit 5 Folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(pipeline_lr, X_train, y_train, cv=kf, scoring=rmspe_scorer, verbose=True)\n",
    "\n",
    "# Durchschnittliche RMSPE aus Cross-Validation\n",
    "mean_rmspe = np.mean(cv_scores)\n",
    "print(f'Mean RMSPE (Cross-Validation): {mean_rmspe}')\n",
    "\n",
    "# 8. Vorhersagen auf dem Testdatensatz machen\n",
    "y_pred = pipeline_lr.predict(X_test)\n",
    "\n",
    "# RMSPE auf Testdatensatz berechnen\n",
    "test_rmspe = rmspe(y_test, y_pred)\n",
    "print(f'RMSPE on Test Data: {test_rmspe}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import make_scorer\n",
    "from prophet import Prophet\n",
    "\n",
    "# Angepasste RMSPE-Funktion, die Tage mit 0 Sales ignoriert\n",
    "def rmspe(y_true, y_pred):\n",
    "    # Nur Fälle berücksichtigen, bei denen y_true nicht 0 ist\n",
    "    mask = y_true != 0\n",
    "    y_true_filtered = y_true[mask]\n",
    "    y_pred_filtered = y_pred[mask]\n",
    "    \n",
    "    return np.sqrt(np.mean(((y_true_filtered - y_pred_filtered) / y_true_filtered) ** 2))\n",
    "\n",
    "# Laden des Datensatzes\n",
    "data_cleaned = \"../data/cleaned_train.csv\"\n",
    "data = pd.read_csv(data_cleaned, delimiter=\",\", encoding=\"latin\", header=0, thousands=\",\", decimal='.', low_memory=False)\n",
    "\n",
    "# Sicherstellen, dass das Datumsformat korrekt ist\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# Umwandeln der Daten in das Prophet-kompatible Format\n",
    "df_prophet = data[['Date', 'Sales']].rename(columns={'Date': 'ds', 'Sales': 'y'})\n",
    "\n",
    "# Aufteilen der Daten in Trainings- und Testsets (Beispiel: Daten vor Juli 2015 zum Trainieren)\n",
    "train_data_prophet = df_prophet[df_prophet['ds'] < '2015-07-01']\n",
    "test_data_prophet = df_prophet[df_prophet['ds'] >= '2015-07-01']\n",
    "\n",
    "# Erstellen eines Prophet-Modells\n",
    "model = Prophet()\n",
    "\n",
    "# Trainieren des Prophet-Modells mit den Trainingsdaten\n",
    "model.fit(train_data_prophet)\n",
    "\n",
    "# Erstellen eines DataFrames für zukünftige Datenpunkte (Vorhersagen für die nächsten Testtage)\n",
    "future = model.make_future_dataframe(periods=len(test_data_prophet), freq='D')\n",
    "\n",
    "# Vorhersagen erstellen\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Vorhersagen anzeigen (yhat ist die vorhergesagte Sales-Wert)\n",
    "print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail())\n",
    "\n",
    "# Zusammenführen der Vorhersagen mit den tatsächlichen Werten\n",
    "forecast_with_actual = forecast[['ds', 'yhat']].merge(test_data_prophet, on='ds')\n",
    "\n",
    "# RMSPE-Bewertung berechnen\n",
    "test_rmspe_prophet = rmspe(forecast_with_actual['y'], forecast_with_actual['yhat'])\n",
    "print(\"Test RMSPE for Prophet:\", test_rmspe_prophet)\n",
    "\n",
    "# Vorhersage-Plot\n",
    "model.plot(forecast)\n",
    "\n",
    "# Komponenten-Plot (Trend, wöchentliche und jährliche Saisonalität)\n",
    "model.plot_components(forecast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Versuch mit Timestamp Cross Validation und ohne lag feature n_neighbors = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durchschnittlicher RMSPE (KNN, TimeSeries Cross-Validation): -0.30225682404536586\n",
      "Test RMSPE (KNN): 0.24405497742782506\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import make_scorer  # make_scorer importieren\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Laden des Datensatzes\n",
    "data_cleaned = \"../data/cleaned_train.csv\"\n",
    "data = pd.read_csv(data_cleaned, delimiter=\",\", encoding=\"latin\", header=0, thousands=\",\", decimal='.', low_memory=False)\n",
    "\n",
    "# Datensatz sortieren, falls nicht bereits geschehen (angenommen, du hast eine Spalte 'Date')\n",
    "data = data.sort_values('Date')\n",
    "\n",
    "# auch noch Text hinzufügen\n",
    "data = data[data ['Open']!=0]\n",
    "data = data[data ['Sales']>0]\n",
    "\n",
    "# Zielvariable und Features definieren\n",
    "X = data.drop(['Sales', 'Date', 'Open'], axis=1)  # 'Date' wird entfernt, wenn es keine erklärende Variable ist\n",
    "y = data['Sales']\n",
    "\n",
    "# Definiere die numerischen und kategorischen Features\n",
    "numerical_features = ['year', 'month', 'day', 'week_of_year']\n",
    "# Bereits encodierte Features\n",
    "already_encoded_features = ['Open', 'Promo', 'promo2']\n",
    "# Noch nicht encodierte kategorische Features\n",
    "categorical_features_to_encode = ['Store', 'DayOfWeek', 'StoreType', 'StateHoliday','Assortment']\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "# Bestimme die Grenze für 80% Training und 20% Test\n",
    "train_size = int(len(df) * 0.8)\n",
    "# Aufteilen in Trainings- und Test-Datensätze\n",
    "train = df.iloc[:train_size]\n",
    "test = df.iloc[train_size:]\n",
    "# Ziel- und Eingabedaten für Training und Test\n",
    "X_train = train.drop('Sales', axis=1)\n",
    "y_train = train['Sales']\n",
    "X_test = test.drop('Sales', axis=1)\n",
    "y_test = test['Sales']\n",
    "\n",
    "# Angepasste RMSPE-Funktion, die Tage mit 0 Sales ignoriert\n",
    "def rmspe(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    y_true_filtered = y_true[mask]\n",
    "    y_pred_filtered = y_pred[mask]\n",
    "    return np.sqrt(np.mean(((y_true_filtered - y_pred_filtered) / y_true_filtered) ** 2))\n",
    "# RMSPE als Scorer definieren\n",
    "rmspe_scorer = make_scorer(rmspe, greater_is_better=False)\n",
    "\n",
    "# Erstelle den Preprocessor für numerische und kategorische Features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features + already_encoded_features),\n",
    "        ('enc', 'passthrough', already_encoded_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features_to_encode)\n",
    "    ])\n",
    "# Erstelle die Pipeline für KNN\n",
    "pipeline_knn = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', KNeighborsRegressor(n_neighbors=2)) # Hyperparameter zum anpassen hoch und runter\n",
    "])\n",
    "\n",
    "\n",
    "# Trainiere das KNN-Modell\n",
    "pipeline_knn.fit(X_train, y_train)\n",
    "# 5-fold TimeSeries Cross-Validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "# Cross-Validation mit TimeSeriesSplit\n",
    "cv_scores_knn = cross_val_score(pipeline_knn, X_train, y_train, cv=tscv, scoring=rmspe_scorer, verbose=True)\n",
    "\n",
    "# Durchschnittlichen RMSPE berechnen\n",
    "mean_rmspe_knn = np.mean(cv_scores_knn)\n",
    "print(\"Durchschnittlicher RMSPE (KNN, TimeSeries Cross-Validation):\", mean_rmspe_knn)\n",
    "# 3. Vorhersagen auf dem Testset machen\n",
    "y_pred_knn = pipeline_knn.predict(X_test)\n",
    "# Test RMSPE berechnen\n",
    "test_rmspe_knn = rmspe(y_test, y_pred_knn)\n",
    "print(\"Test RMSPE (KNN):\", test_rmspe_knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Durchschnittlicher RMSPE ist schlechter geworden (von -0.184 auf -0.232)\n",
    "**Was es bedeutet**:\n",
    "- Der RMSPE-Wert in der zeitbasierten Cross-Validation ist höher geworden. Das bedeutet, dass das Modell auf den Trainingsdaten in der zeitlichen Cross-Validation schlechter abschneidet als zuvor in der regulären Cross-Validation.\n",
    "\n",
    "**Gründe für die Verschlechterung:**\n",
    "- Zeitabhängigkeit der Daten: Die Daten, die du modellierst, haben eine zeitliche Komponente, und das Modell hat Schwierigkeiten, die Muster über die Zeit hinweg zu verallgemeinern. Die nicht-zeitliche KFold-Cross-Validation vermischt die Zeiträume, was möglicherweise eine unrealistisch bessere Leistung ergab. Jetzt, wo die Cross-Validation die Reihenfolge respektiert, zeigt sich, dass das Modell nicht so gut verallgemeinert.\n",
    "**Trainingsmengen:** \n",
    "- Bei zeitbasierter Cross-Validation wird das Modell in den ersten Folds mit einer kleineren Datenmenge trainiert (weil frühere Zeitpunkte für das Training und spätere für das Testen verwendet werden). Das kann zu schlechteren Ergebnissen führen, da das Modell mit weniger Daten trainiert wird.\n",
    "\n",
    "\n",
    "## 2. Vergleich mit dem Test RMSPE (0.253)\n",
    "\n",
    "- Der RMSPE aus der zeitbasierten Cross-Validation (0.232) liegt immer noch unter dem RMSPE auf dem Testset (0.253), was bedeutet, dass das Modell immer noch tendenziell schlechter auf den Testdaten abschneidet, aber die Werte sind näher beieinander.\n",
    "\n",
    "**Was das bedeutet:** \n",
    "- Die zeitbasierte Cross-Validation scheint jetzt eine realistischere Abschätzung der Modellleistung zu liefern, da die Abweichung zwischen dem RMSPE aus der Cross-Validation und dem Test-RMSPE kleiner geworden ist. Dies deutet darauf hin, dass die nicht-zeitliche Cross-Validation das Modell möglicherweise überoptimistisch bewertet hat. (Overfitting?)\n",
    "\n",
    "\n",
    "## Fazit:\n",
    "- Die Verschlechterung des RMSPE nach Einführung der zeitbasierten Cross-Validation ist ein Hinweis darauf, dass das Modell Schwierigkeiten hat, Muster in zeitlich sortierten Daten zu lernen.\n",
    "- Der Unterschied zwischen der Cross-Validation-Leistung (0.232) und der Testleistung (0.253) ist jedoch kleiner geworden, was bedeutet, dass die zeitbasierte Cross-Validation eine realistischere Abschätzung der Modellleistung bietet.\n",
    "- Möglicherweise musst du das Modell weiter verbessern, insbesondere um die Zeitabhängigkeit der Daten besser zu erfassen, z. B. durch Feature-Engineering (Hinzufügen von Zeittrends oder saisonalen Mustern) oder durch den Einsatz von Modellen, die sich besser für zeitliche Daten eignen, wie z. B. ARIMA, Prophet, oder Recurrent Neural Networks (RNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Versuch Hyperparametertuning (n_neighbors = 6) und Feature Engineering (testen von versch. features aus kaggle) zur Verbesserung des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durchschnittlicher RMSPE (KNN, TimeSeries Cross-Validation): -0.2794781299440422\n",
      "Test RMSPE (KNN): 0.22253414113163228\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Laden des Datensatzes\n",
    "data_cleaned = \"../data/cleaned_train.csv\"\n",
    "data = pd.read_csv(data_cleaned, delimiter=\",\", encoding=\"latin\", header=0, thousands=\",\", decimal='.', low_memory=False)\n",
    "# Datensatz sortieren, falls nicht bereits geschehen (angenommen, du hast eine Spalte 'Date')\n",
    "data = data.sort_values('Date')\n",
    "\n",
    "# auch noch Text hinzufügen\n",
    "data = data[data ['Open']!=0]\n",
    "data = data[data ['Sales']>0]\n",
    "\n",
    "# Zielvariable und Features definieren\n",
    "X = data.drop(['Sales', 'Date', 'Open'], axis=1)  # 'Date' wird entfernt, wenn es keine erklärende Variable ist\n",
    "y = data['Sales']\n",
    "# Definiere die numerischen und kategorischen Features\n",
    "numerical_features = ['year', 'month', 'day', 'week_of_year']\n",
    "# Bereits encodierte Features\n",
    "already_encoded_features = ['Open', 'Promo', 'promo2']\n",
    "# Noch nicht encodierte kategorische Features\n",
    "categorical_features_to_encode = ['Store', 'DayOfWeek', 'StoreType', 'StateHoliday','Assortment']\n",
    "\n",
    "# 1. Datensatz aufteilen in Trainings- und Testdaten\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Bestimme die Grenze für 80% Training und 20% Test\n",
    "train_size = int(len(df) * 0.8)\n",
    "# Aufteilen in Trainings- und Test-Datensätze\n",
    "train = df.iloc[:train_size]\n",
    "test = df.iloc[train_size:]\n",
    "# Ziel- und Eingabedaten für Training und Test\n",
    "X_train = train.drop('Sales', axis=1)\n",
    "y_train = train['Sales']\n",
    "X_test = test.drop('Sales', axis=1)\n",
    "y_test = test['Sales']\n",
    "\n",
    "# Angepasste RMSPE-Funktion, die Tage mit 0 Sales ignoriert\n",
    "def rmspe(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    y_true_filtered = y_true[mask]\n",
    "    y_pred_filtered = y_pred[mask]\n",
    "    return np.sqrt(np.mean(((y_true_filtered - y_pred_filtered) / y_true_filtered) ** 2))\n",
    "# RMSPE als Scorer definieren\n",
    "rmspe_scorer = make_scorer(rmspe, greater_is_better=False)\n",
    "\n",
    "# Erstelle den Preprocessor für numerische und kategorische Features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features + already_encoded_features),\n",
    "        ('enc', 'passthrough', already_encoded_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features_to_encode)\n",
    "    ])\n",
    "# Erstelle die Pipeline für KNN\n",
    "pipeline_knn = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', KNeighborsRegressor(n_neighbors=6))  #hier n_neighbors anpassen\n",
    "])\n",
    "\n",
    "# Hyperparameter-Tuning mit GridSearchCV\n",
    "param_grid = {\n",
    "    'model__n_neighbors': [3, 5, 10, 15],   # Verschiedene Werte für k\n",
    "    'model__weights': ['uniform', 'distance'],  # Gewichtete oder uniforme Nachbarn\n",
    "    'model__metric': ['euclidean', 'manhattan']  # Verschiedene Distanzmetriken\n",
    "}\n",
    "# Trainiere das KNN-Modell\n",
    "pipeline_knn.fit(X_train, y_train)\n",
    "# 5-fold TimeSeries Cross-Validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "# Cross-Validation mit TimeSeriesSplit\n",
    "cv_scores_knn = cross_val_score(pipeline_knn, X_train, y_train, cv=tscv, scoring=rmspe_scorer, verbose=True)\n",
    "\n",
    "# Durchschnittlichen RMSPE berechnen\n",
    "mean_rmspe_knn = np.mean(cv_scores_knn)\n",
    "print(\"Durchschnittlicher RMSPE (KNN, TimeSeries Cross-Validation):\", mean_rmspe_knn)\n",
    "# 3. Vorhersagen auf dem Testset machen\n",
    "y_pred_knn = pipeline_knn.predict(X_test)\n",
    "# Test RMSPE berechnen\n",
    "test_rmspe_knn = rmspe(y_test, y_pred_knn)\n",
    "print(\"Test RMSPE (KNN):\", test_rmspe_knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versuch mit Feature wie CompetitionOpen und Promo2Open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden des Datensatzes\n",
    "data_cleaned = \"../data/cleaned_train.csv\"\n",
    "data = pd.read_csv(data_cleaned, delimiter=\",\", encoding=\"latin\", header=0, thousands=\",\", decimal='.', low_memory=False)\n",
    "# Datensatz sortieren, falls nicht bereits geschehen (angenommen, du hast eine Spalte 'Date')\n",
    "data = data.sort_values('Date')\n",
    "# auch noch Text hinzufügen\n",
    "data = data[data ['Open']!=0]\n",
    "data = data[data ['Sales']>0]\n",
    "# Zielvariable und Features definieren\n",
    "X = data.drop(['Sales', 'Date', 'Open'], axis=1)  # 'Date' wird entfernt, wenn es keine erklärende Variable ist\n",
    "y = data['Sales']\n",
    "# Definiere die numerischen und kategorischen Features\n",
    "numerical_features = ['year', 'month', 'day', 'week_of_year']\n",
    "# Bereits encodierte Features\n",
    "already_encoded_features = ['Open', 'Promo', 'promo2']\n",
    "# Noch nicht encodierte kategorische Features\n",
    "categorical_features_to_encode = ['Store', 'DayOfWeek', 'StoreType', 'StateHoliday','Assortment']\n",
    "# 1. Datensatz aufteilen in Trainings- und Testdaten\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hier n_neighbors anpassen und GridSearch/Randomized Search/ Optuna definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iTTaste\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Laden des Datensatzes\n",
    "data_cleaned = \"../data/cleaned_train.csv\"\n",
    "data = pd.read_csv(data_cleaned, delimiter=\",\", encoding=\"latin\", header=0, thousands=\",\", decimal='.', low_memory=False)\n",
    "# Datensatz sortieren, falls nicht bereits geschehen (angenommen, du hast eine Spalte 'Date')\n",
    "data = data.sort_values('Date')\n",
    "\n",
    "# auch noch Text hinzufügen\n",
    "data = data[data ['Open']!=0]\n",
    "data = data[data ['Sales']>0]\n",
    "\n",
    "# Zielvariable und Features definieren\n",
    "X = data.drop(['Sales', 'Date', 'Open'], axis=1)  # 'Date' wird entfernt, wenn es keine erklärende Variable ist\n",
    "y = data['Sales']\n",
    "# Definiere die numerischen und kategorischen Features\n",
    "numerical_features = ['year', 'month', 'day', 'week_of_year']\n",
    "# Bereits encodierte Features\n",
    "already_encoded_features = ['Open', 'Promo', 'promo2']\n",
    "# Noch nicht encodierte kategorische Features\n",
    "categorical_features_to_encode = ['Store', 'DayOfWeek', 'StoreType', 'StateHoliday','Assortment']\n",
    "\n",
    "# 1. Datensatz aufteilen in Trainings- und Testdaten\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Bestimme die Grenze für 80% Training und 20% Test\n",
    "train_size = int(len(df) * 0.8)\n",
    "# Aufteilen in Trainings- und Test-Datensätze\n",
    "train = df.iloc[:train_size]\n",
    "test = df.iloc[train_size:]\n",
    "# Ziel- und Eingabedaten für Training und Test\n",
    "X_train = train.drop('Sales', axis=1)\n",
    "y_train = train['Sales']\n",
    "X_test = test.drop('Sales', axis=1)\n",
    "y_test = test['Sales']\n",
    "\n",
    "# Angepasste RMSPE-Funktion, die Tage mit 0 Sales ignoriert\n",
    "def rmspe(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    y_true_filtered = y_true[mask]\n",
    "    y_pred_filtered = y_pred[mask]\n",
    "    return np.sqrt(np.mean(((y_true_filtered - y_pred_filtered) / y_true_filtered) ** 2))\n",
    "# RMSPE als Scorer definieren\n",
    "rmspe_scorer = make_scorer(rmspe, greater_is_better=False)\n",
    "\n",
    "# Erstelle den Preprocessor für numerische und kategorische Features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features + already_encoded_features),\n",
    "        ('enc', 'passthrough', already_encoded_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features_to_encode)\n",
    "    ])\n",
    "# Erstelle die Pipeline für KNN\n",
    "pipeline_knn = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', KNeighborsRegressor(n_neighbors=6))  #hier n_neighbors anpassen\n",
    "])\n",
    "\n",
    "# Hyperparameter-Tuning mit GridSearchCV / RandomizedCV / Optuna\n",
    "param_grid = {\n",
    "    'model__n_neighbors': [3, 5, 10, 15],   # Verschiedene Werte für k\n",
    "    'model__weights': ['uniform', 'distance'],  # Gewichtete oder uniforme Nachbarn\n",
    "    'model__metric': ['euclidean', 'manhattan']  # Verschiedene Distanzmetriken\n",
    "}\n",
    "# Trainiere das KNN-Modell\n",
    "pipeline_knn.fit(X_train, y_train)\n",
    "# 5-fold TimeSeries Cross-Validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "# Cross-Validation mit TimeSeriesSplit\n",
    "cv_scores_knn = cross_val_score(pipeline_knn, X_train, y_train, cv=tscv, scoring=rmspe_scorer, verbose=True)\n",
    "\n",
    "\n",
    "# Definiere die Optuna-Ziel-Funktion\n",
    "def objective(trial):\n",
    "    # Hyperparameter, die Optuna optimieren soll\n",
    "    n_neighbors = trial.suggest_int('n_neighbors', 2, 20)  # Optimierung von n_neighbors zwischen 2 und 20\n",
    "    weights = trial.suggest_categorical('weights', ['uniform', 'distance'])  # 'uniform' oder 'distance'\n",
    "    p = trial.suggest_int('p', 1, 2)  # Metrik: 1 = Manhattan, 2 = Euklidische Distanz\n",
    "    # KNN Modell mit den von Optuna vorgeschlagenen Parametern\n",
    "    model = KNeighborsRegressor(n_neighbors=n_neighbors, weights=weights, p=p) \n",
    "    # Pipeline, die den Preprocessor und das KNN-Modell enthält\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])   \n",
    "    # Cross-Validation\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=tscv, scoring=rmspe_scorer, n_jobs=-1)\n",
    "    # Durchschnittlicher RMSPE (negative Werte, weil 'greater_is_better=False')\n",
    "    mean_rmspe = np.mean(cv_scores)\n",
    "    return mean_rmspe  # Minimierung des RMSPE\n",
    "# Optuna-Studie starten\n",
    "study = optuna.create_study(direction='minimize')  # Wir wollen den Fehler minimieren\n",
    "study.optimize(objective, n_trials=50)  # Probiere 50 verschiedene Hyperparameter-Kombinationen aus\n",
    "# Beste Parameter anzeigen\n",
    "print(\"Beste Parameter: \", study.best_params)\n",
    "# Trainiere das Modell mit den besten Parametern\n",
    "best_n_neighbors = study.best_params['n_neighbors']\n",
    "best_weights = study.best_params['weights']\n",
    "best_p = study.best_params['p']\n",
    "# Bestes Modell trainieren\n",
    "best_model = KNeighborsRegressor(n_neighbors=best_n_neighbors, weights=best_weights, p=best_p)\n",
    "pipeline_best = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', best_model)\n",
    "])\n",
    "pipeline_best.fit(X_train, y_train)\n",
    "# Vorhersagen auf dem Testset\n",
    "y_pred_best = pipeline_best.predict(X_test)\n",
    "test_rmspe_best = rmspe(y_test, y_pred_best)\n",
    "\n",
    "print(\"Test RMSPE (bestes KNN-Modell):\", test_rmspe_best)\n",
    "\n",
    "\n",
    "\n",
    "# Durchschnittlichen RMSPE berechnen\n",
    "mean_rmspe_knn = np.mean(cv_scores_knn)\n",
    "print(\"Durchschnittlicher RMSPE (KNN, TimeSeries Cross-Validation):\", mean_rmspe_knn)\n",
    "# 3. Vorhersagen auf dem Testset machen\n",
    "y_pred_knn = pipeline_knn.predict(X_test)\n",
    "# Test RMSPE berechnen\n",
    "test_rmspe_knn = rmspe(y_test, y_pred_knn)\n",
    "print(\"Test RMSPE (KNN):\", test_rmspe_knn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
