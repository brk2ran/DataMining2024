{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from xgboost import plot_importance, plot_tree\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Angepasste RMSPE-Funktion, die Tage mit 0 Sales ignoriert\n",
    "def rmspe(y_true, y_pred):\n",
    "    # Nur Fälle berücksichtigen, bei denen y_true nicht 0 ist\n",
    "    mask = y_true != 0\n",
    "    y_true_filtered = y_true[mask]\n",
    "    y_pred_filtered = y_pred[mask]\n",
    "    \n",
    "    return np.sqrt(np.mean(((y_true_filtered - y_pred_filtered) / y_true_filtered) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSPE als Scorer definieren\n",
    "rmspe_scorer = make_scorer(rmspe, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeSeries Kreuzvalidierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "# Zeitreihen-Kreuzvalidierung einrichten\n",
    "tscv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten einlesen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Laden des Datensatzes\n",
    "data_cleaned = \"../data/cleaned_train.csv\"\n",
    "data = pd.read_csv(data_cleaned, delimiter=\",\", encoding=\"latin\", header=0, thousands=\",\", decimal='.', low_memory=False)\n",
    "\n",
    "# Definiere die numerischen und kategorischen Features\n",
    "numerical_features = ['year', 'month', 'day', 'week_of_year', 'fourier_sin_365', 'fourier_cos_365',\t'days_since_last_holiday',\t'days_until_next_holiday'] # lag_1, lag_7 entfernt\n",
    "\n",
    "# Bereits encodierte Features\n",
    "already_encoded_features = ['Open', 'Promo', 'promo2']\n",
    "\n",
    "# Noch nicht encodierte kategorische Features\n",
    "categorical_features_to_encode = ['DayOfWeek', 'StoreType', 'StateHoliday','Assortment', 'Store'] # 'Store' entfernt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der folgende Code beschreibt die Aufteilung der Daten in einen Trainings- und Testdatensatz zur Vorbereitung für maschinelles Lernen. Zunächst wird der Datensatz anhand der Spalte „Date“ sortiert, um sicherzustellen, dass die zeitliche Reihenfolge der Datenpunkte korrekt ist. Dies ist  notwendig,da es sich um eine Zeitreihenanalyse handelt, bei der die Reihenfolge der Daten für die Genauigkeit der Vorhersage entscheidend ist.\n",
    "\n",
    "Im nächsten Schritt wird die Zielvariable „Sales“ festgelegt. Sie wird separat als y definiert, da sie die zu prognostizierenden Werte enthält. Gleichzeitig werden die Spalten „Sales“ und „Date“ aus den Features (X) entfernt, da „Sales“ die Zielgröße ist und „Date“ in diesem Kontext keine erklärende Variable darstellt. Dies hilft, das Modell auf die relevanten Merkmale des Datensatzes zu reduzieren.\n",
    "\n",
    "Zuletzt erfolgt die Aufteilung des Datensatzes in Trainings- und Testdaten. Die ersten 80 % der Daten werden für das Training des Modells verwendet, während die letzten 20 % als Testdaten reserviert werden, um das Modell auf noch nicht gesehenen Daten zu evaluieren. Dieser Ansatz, bei dem die Testdaten aus den letzten Datenpunkten bestehen, ist besonders sinnvoll für zeitbasierte Modelle, da zukünftige Werte auf Grundlage vergangener Ereignisse vorhergesagt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datensatz sortieren, falls nicht bereits geschehen (angenommen, du hast eine Spalte 'Date')\n",
    "data = data.sort_values('Date')\n",
    "\n",
    "# ...\n",
    "data = data[data ['Open']!=0]\n",
    "data = data[data ['Sales']>0]\n",
    "\n",
    "# Zielvariable und Features definieren\n",
    "X = data.drop(['Sales', 'Date', 'Open'], axis=1)  # 'Date' wird entfernt, wenn es keine erklärende Variable ist\n",
    "y = data['Sales']\n",
    "\n",
    "# Berechnung der Anzahl der Testdaten (20 % des Datensatzes)\n",
    "test_size = int(len(data) * 0.2)\n",
    "\n",
    "# Aufteilen der Daten in Trainings- und Testdaten\n",
    "X_train, X_test = X.iloc[:-test_size], X.iloc[-test_size:]\n",
    "y_train, y_test = y.iloc[:-test_size], y.iloc[-test_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Preprocessing wird ein ColumnTransformer verwendet, um numerische, bereits encodierte und noch nicht encodierte kategoriale Merkmale eines Datensatzes gezielt zu transformieren. Dieser Schritt ist essenziell, um eine konsistente und effektive Vorbereitung der Daten zu gewährleisten.\n",
    "\n",
    "Die numerischen Merkmale, die in der Liste numerical_features enthalten sind, umfassen Variablen wie year, month, day, week_of_year, sowie Lag-Features (lag_1, lag_7, lag_30), Fourier-Transformationen zur Erfassung saisonaler Zyklen und Variablen zur Berücksichtigung von Feiertagen. Diese werden mit dem StandardScaler standardisiert, um sicherzustellen, dass alle numerischen Werte auf einen einheitlichen Maßstab gebracht werden.\n",
    "\n",
    "Bereits One-Hot-encodierte Merkmale (already_encoded_features) wie Open, Promo und promo2 werden ohne weitere Transformation mit 'passthrough' durchgeschleust. Diese Variablen wurden entweder schon in vorherigen Schritten in eine geeignete Form gebracht oder bedürfen keiner weiteren Skalierung oder Encodierung.\n",
    "\n",
    "Die noch nicht encodierten kategorialen Merkmale, wie DayOfWeek, StoreType, StateHoliday und Assortment, werden mittels OneHotEncoder in binäre Merkmale umgewandelt. Dies gewährleistet, dass jedes Merkmal durch eine Reihe von Spalten dargestellt wird, die angeben, ob eine bestimmte Kategorie vorliegt. Um potenziell unbekannte Kategorien in den Testdaten zu berücksichtigen, wird die Option handle_unknown='ignore' verwendet, wodurch unbekannte Werte ignoriert werden, ohne einen Fehler zu verursachen.\n",
    "\n",
    "Durch diesen modularen Ansatz werden die numerischen und kategorialen Features des Datensatzes effizient für das Training des Machine-Learning-Modells vorbereitet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle den Preprocessor für numerische und kategorische Features (ohne Datumsextraktion)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),  # Skalierung für numerische und bereits encodierte Features\n",
    "        ('enc', 'passthrough', already_encoded_features),  # Bereits encodierte Features durchschleusen (keine weitere Transformation)\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features_to_encode)  # Nur noch nicht encodierte Features encodieren\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lag-Features (bitte alle ausprobieren) - nachher bei jedem im eigenen Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #pipeline mit preprocessor & model definieren\n",
    "# Pipeline für die Modellierung\n",
    "pipeline  = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # Schritt 1: Vorverarbeitung\n",
    "    ('model', ... )  # Schritt 3: Modellierung\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verbesserte iterative Vorhersagefunktion (mit Berücksichtigung von Lag- und zusätzlichen Features)\n",
    "def iterative_forecast(pipeline, X_fold_train, y_fold_train, X_fold_val):\n",
    "    # Trainiere das Modell mit allen Features\n",
    "    pipeline.named_steps['model'].fit(X_fold_train, y_fold_train)\n",
    "    \n",
    "    # Vorhersagen auf Basis der Testdaten\n",
    "    predictions = []\n",
    "    last_known_sales = list(y_fold_train[-7:])  # Letzte 30 Tage tatsächlicher Sales-Daten\n",
    "    \n",
    "    # Iteriere über die Zeilen von X_fold_val\n",
    "    for index, row in X_fold_val.iterrows():\n",
    "        try:\n",
    "            # Lag-Features für den nächsten Tag berechnen\n",
    "            row[\"lag_1\"] = last_known_sales[-1]  # Lag-1 ist der zuletzt vorhergesagte Wert\n",
    "            row[\"lag_7\"] = last_known_sales[-7] if len(last_known_sales) >= 7 else np.nan  # Lag-7 ist 7 Tage zurück\n",
    "            #row[\"lag_30\"] = last_known_sales[-30] if len(last_known_sales) >= 30 else np.nan  # Lag-30 ist 30 Tage zurück\n",
    "        except IndexError as e:\n",
    "            print(f\"Error in creating lag features for row {index}: {e}\")\n",
    "            row[\"lag_1\"] = np.nan\n",
    "            row[\"lag_7\"] = np.nan\n",
    "            #row[\"lag_30\"] = np.nan\n",
    "        \n",
    "        # Wandeln der Zeile in einen DataFrame, um den Preprocessing-Schritt durchzuführen\n",
    "        row = row.to_frame().T\n",
    "        \n",
    "        # Wende das Preprocessing an\n",
    "        row_preprocessed = pipeline.named_steps['preprocessor'].transform(row)\n",
    "        \n",
    "        # Vorhersage für den nächsten Tag\n",
    "        pred = pipeline.named_steps['model'].predict(row_preprocessed)\n",
    "        predictions.append(pred[0])  # pred ist ein Array, daher pred[0]\n",
    "        \n",
    "        # Update der letzten bekannten Sales-Daten\n",
    "        last_known_sales.append(pred[0])\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Durchführung der Kreuzvalidierung auf den Trainingsdaten\n",
    "rmspe_scores = []  # Liste zur Speicherung der RMSPE-Scores\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(X_train)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    \n",
    "    # Train- und Testdaten für diesen Fold\n",
    "    X_fold_train = X_train.iloc[train_index]\n",
    "    X_fold_test = X_train.iloc[test_index]\n",
    "    y_fold_train = y_train.iloc[train_index]\n",
    "    y_fold_test = y_train.iloc[test_index]\n",
    "    \n",
    "    # Preprocessing auf die Train- und Testdaten anwenden\n",
    "    X_fold_train_preprocessed = pipeline.named_steps['preprocessor'].fit_transform(X_fold_train)\n",
    "    X_fold_test_preprocessed = pipeline.named_steps['preprocessor'].transform(X_fold_test)\n",
    "    \n",
    "    # Iterative Vorhersage über die Testperiode\n",
    "    y_pred = iterative_forecast(pipeline, X_fold_train_preprocessed, y_fold_train, X_fold_test)\n",
    "    \n",
    "    # RMSPE berechnen und speichern\n",
    "    rmspe_score = rmspe(y_fold_test, y_pred)\n",
    "    rmspe_scores.append(rmspe_score)\n",
    "    \n",
    "    print(f\"RMSPE for Fold {fold + 1}: {rmspe_score}\")\n",
    "\n",
    "# Gesamtergebnisse\n",
    "print(f\"Durchschnittliche RMSPE über alle Folds: {np.mean(rmspe_scores)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
